# peft

| Model            | PEFT method      | #Params (M) | Trainable params (M) | Trainable ratio | VRAM peak (MB) | CoLA | SST2 | MRPC (Acc/F1) | STS-B (P/S) | QQP (Acc/F1) | MNLI (m/mm) | QNLI | RTE |
|------------------|------------------|-------------|----------------------|-----------------|----------------|------|------|----------------|-------------|--------------|-------------|------|-----|
| RoBERTa-base     | FT               | 124.65      |                      |                 | 3997           | 58.80 | 94.15 | 87.25/90.88    | 89.58/89.51 | 91.25/88.36  | 87.54/87.27 | 92.70 | 68.95 |
| RoBERTa-base     | LoRA qkv         | 125.68      | 1.03                 | 0.8231%         | 2689           | 0.00  | 92.32 | 68.38/81.22    | 55.16/57.68 | 85.84/81.64  | 83.13/83.87 | 88.56 | 52.71 |
| RoBERTa-base     | LoRA all layers  | 126.57      | 1.92                 | 1.5164%         | 3784           | 33.69 | 93.00 | 68.38/81.22    | 80.81/81.76 | 87.79/84.20  | 85.13/85.50 | 89.97 | 53.07 |
| RoBERTa-large    | FT               | 355.36      |                      |                 | 10627          | 62.58 | 96.10 | 89.46/92.60    | 92.29/92.02 | 91.71/88.97  | 90.29/90.16 | 93.04 | 55.96 |
| RoBERTa-large    | LoRA qkv         | 357.59      | 2.23                 | 0.6240%         | 7069           | 10.39 | 94.84 | 68.38/81.22    | 12.64/12.10 | 87.93/84.32  | 89.10/88.91 | 91.58 | 48.74 |
| RoBERTa-large    | LoRA all layers  | 359.95      | 4.59                 | 1.2753%         | 9987           | 52.46 | 95.53 | 68.38/81.22    | 19.27/18.80 | 89.38/86.03  | 90.07/89.73 | 93.47 | 53.79 |
| DeBERTaV3-base   | FT               | 184.42      |                      |                 | 8953           | 67.00 | 95.76 | 88.73/91.79    | 90.47/90.27 | 92.30/89.71  | 90.64/90.54 | 94.16 | 80.14 |
| DeBERTaV3-base   | LoRA qkv         | 184.87      | 0.44                 | 0.2401%         | 7183           | 0.00  | 92.89 | 68.38/81.22    | 23.03/22.34 | 87.50/83.65  | 87.72/88.00 | 90.12 | 47.29 |
| DeBERTaV3-base   | LoRA all layers  | 185.76      | 1.34                 | 0.7218%         | 8276           | 58.04 | 94.72 | 68.38/81.22    | 77.07/79.66 | 89.12/85.64  | 89.12/89.00 | 92.02 | 47.29 |
| RoBERTa-base | QLoRA qkv          | 83.21       | 1.03                  | 1.2432%         | 5585           | 0.00 | 92.09 | 68.38/81.22    | 16.71/23.23 | 84.93/80.77  | 82.20/82.90  | 87.30 | 52.71 |
| RoBERTa-base | QLoRA all layers   | 84.10       | 1.92                  | 2.2821%         | 7759           | 0.00 | 92.43 | 68.38/81.22    | 55.31/57.95 | 87.02/82.86  | 84.23/84.81  | 89.38 | 52.71 |