# TODO List


## 1. Slides
- [x] LoRA methodology
- [ ] QLoRA methodology
- [ ] Discussion

## 2. Experiments
### 2.1 Full finetuning
- [x] RoBERTa-base fp32 | https://www.kaggle.com/code/vohoangg/peft-fft | P100 | version 1 
- [x] RoBERTa-base fp16 | https://www.kaggle.com/code/hoanggvo/fork-of-ft-peft | P100 | version 10
- [x] RoBERTa-base bp16 | https://www.kaggle.com/code/hoanggvo/fork-of-ft-peft | P100 | version 12 
- [x] RoBERTa-base fp16 | https://www.kaggle.com/code/hoanggvo/fork-of-ft-peft | T4   | version 14


### 2.2 LoRA
#### 2.2.1 LoRA: q, k, v
- [x] RoBERTa-base | https://www.kaggle.com/code/nlp02aio/peft-lora | version 1
- [x] RoBERTa-large | https://www.kaggle.com/code/vohoangsfas/peft-lora | version 3 
- [ ] DeBERTa-v3 | https://www.kaggle.com/code/shiroyyyyya/peft-lora

#### 2.2.2 LoRA: q, k, v, d (attention) 
- [ ] RoBERTa-base | https://www.kaggle.com/code/lemmnguyen/peft-lora
- [ ] RoBERTa-large|  https://www.kaggle.com/code/hoanggv/peft-lora
- [ ] DeBERTa-v3 | https://www.kaggle.com/code/nlp02aio/peft-lora 

### 2.3 QLoRA


